{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "348cd3e9-2dc0-427e-8bde-15649ba8a27e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/movie_rv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6874de0-ac98-4590-b888-c735430a1aa1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 텍스트 전처리\n",
    "- Python 문법으로 진행한 것은 > 데이터 정제하는 과정, 특수문자/기타 공백 제거\n",
    "- 정제 작업을 한 후에 한글/영어에 따라 전처리 진행\n",
    "- nltk 패키지로 학습\n",
    "- 토큰화 : 주어진 텍스트 단위를 나누는 작업\n",
    "    - 문장 토큰화\n",
    "    - 단어 토큰화 : 일반적인 토큰화작업\n",
    "\n",
    "- 정규화: go goes (주어에 따라 달라지는) 이런 것들을 하나로 정규화시켜서 통일해야 한다.\n",
    "    - 어간 추출, 표제어 추출\n",
    "    \n",
    "- 품사 태깅 : 명사, 대명사, 형용사 등으로 태깅하여 원하는 분석 요구에 따라 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f66ed7a-4b2e-43c0-bb93-279c7660aa7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d423d7a5-0b39-4261-a51c-a122c2bc271b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kimsinwoo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     /Users/kimsinwoo/nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kimsinwoo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kimsinwoo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kimsinwoo/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('webtext')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec35d6-55de-4fef-9aa7-95222788c18d",
   "metadata": {},
   "source": [
    "### 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d843a85f-e93b-403c-98a1-329f2375cb62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize # 문장 토큰을 쉽게 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2700cb5b-de52-469d-b555-891e35159c0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sp1 = 'Machine Learning is a field of artificial intelligence where computers learn from data to discover patterns and perform tasks without being explicitly programmed. It enables computers to learn and improve from experience automatically. In general, the process of machine learning involves'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abe8ed98-5d3e-43a7-96a2-6fb21b11fa72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine Learning is a field of artificial intelligence where computers learn from data to discover patterns and perform tasks without being explicitly programmed.', 'It enables computers to learn and improve from experience automatically.', 'In general, the process of machine learning involves']\n"
     ]
    }
   ],
   "source": [
    "# 주어진 문장 단위로 토큰\n",
    "print(sent_tokenize(sp1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13798693-7134-4e31-bf60-d7c7a0b24734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b35d4092-4753-4031-b7c9-77e71cb50d6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine', 'Learning', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'where', 'computers', 'learn', 'from', 'data', 'to', 'discover', 'patterns', 'and', 'perform', 'tasks', 'without', 'being', 'explicitly', 'programmed', '.', 'It', 'enables', 'computers', 'to', 'learn', 'and', 'improve', 'from', 'experience', 'automatically', '.', 'In', 'general', ',', 'the', 'process', 'of', 'machine', 'learning', 'involves']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(sp1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4e11d4-0a38-4c22-99e5-40e7a316b081",
   "metadata": {},
   "source": [
    "- 느낌표, 기타 분리등 다양하게 분리할 수 있는 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2236fb68-35ac-4e15-a45e-4839a04ebf9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer #특수문자 제거도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d473cfcf-ae5a-4ed0-82b2-6bed77922e26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine', 'Learning', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'where', 'computers', 'learn', 'from', 'data', 'to', 'discover', 'patterns', 'and', 'perform', 'tasks', 'without', 'being', 'explicitly', 'programmed', '.', 'It', 'enables', 'computers', 'to', 'learn', 'and', 'improve', 'from', 'experience', 'automatically', '.', 'In', 'general', ',', 'the', 'process', 'of', 'machine', 'learning', 'involves']\n"
     ]
    }
   ],
   "source": [
    "print(WordPunctTokenizer().tokenize(sp1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fede1f9c-bcc0-40b3-a800-2c04429230f7",
   "metadata": {},
   "source": [
    "### 정규표현식 토크나이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e65fd3c-d2ce-4b01-9eee-de1b8d442e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d729a627-76e9-456b-9d87-dd62487aebfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('[\\W\"]+') # 정규표현식 제거 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e0306f7-86f2-46c2-93a7-81a606437ba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(tokenizer.tokenize(sp1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7407705b-2d0a-44e8-820d-bedfbb03fa82",
   "metadata": {},
   "source": [
    "### 노이즈 또는 불용어 제거하기\n",
    "- 정규표현식으로 제거할 수 없는 패턴들이 있다.\n",
    "- 특수문자와는 다르게 분석에 의미가 없는 단어들을 말한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27adfabd-19a3-463f-bee1-5bfb1a6cff77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords # 불용어 가져오는 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1d8b302-6384-4d15-8808-399675826da5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "en_stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3ae0dab-e57a-40cb-be47-75a7fdfd1b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c61ae970-1b1d-4e3c-a635-dd574f4ac45e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('[\\W\"]+')\n",
    "tokens = tokenizer.tokenize(sp1.lower()) # 소문자로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37697513-0580-4472-96f0-47fb79246fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = [word for word in tokens if word not in en_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2c32475-02a7-4c16-b17e-138b25c41a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " '. ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " '. ',\n",
       " ' ',\n",
       " ', ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72259b0-0677-45f7-82c6-b948ceeb790b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 정규화 작업\n",
    "- 동사 변환 되는 것\n",
    "- 흔희 어간(stem) 변하지 않는 부분인 어간을 추출한다는 개념\n",
    "- 간다, 갔다, 작다, 작으니, 작고,작아서 .. 작다의 개념에서 파생\n",
    "- 포터 스테머 정규화하는 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15f00e2a-78de-44ce-a2c5-4bf80f2827bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go go goe\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer # 정규화하는 패키지\n",
    "\n",
    "stem = PorterStemmer()\n",
    "print(stem.stem('go'),stem.stem('going'),stem.stem('goes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "662ff91b-155a-4b2b-a0b0-bc9fcdfd8db0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize(sp1)\n",
    "\n",
    "#불용어 한 것처럼 반복문으로 진행\n",
    "res1 = [stem.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a144dc9a-571c-48b9-b241-e626f34aa141",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machin',\n",
       " 'learn',\n",
       " 'is',\n",
       " 'a',\n",
       " 'field',\n",
       " 'of',\n",
       " 'artifici',\n",
       " 'intellig',\n",
       " 'where',\n",
       " 'comput',\n",
       " 'learn',\n",
       " 'from',\n",
       " 'data',\n",
       " 'to',\n",
       " 'discov',\n",
       " 'pattern',\n",
       " 'and',\n",
       " 'perform',\n",
       " 'task',\n",
       " 'without',\n",
       " 'be',\n",
       " 'explicitli',\n",
       " 'program',\n",
       " '.',\n",
       " 'it',\n",
       " 'enabl',\n",
       " 'comput',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'and',\n",
       " 'improv',\n",
       " 'from',\n",
       " 'experi',\n",
       " 'automat',\n",
       " '.',\n",
       " 'in',\n",
       " 'gener',\n",
       " ',',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'involv']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50385d2b-5c7c-4db1-8710-d2e6ce76409b",
   "metadata": {},
   "source": [
    "- 랭카스터 스테마\n",
    "- 결과는 포터 스테마랑은 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddd04279-3acd-42d2-887c-909db4b8a726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3039ddd9-c4f2-443f-a0a5-7184af3a3f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lstem = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b95ef330-1f59-4b5b-9ef9-8347f90e2a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go going goe\n"
     ]
    }
   ],
   "source": [
    "print(lstem.stem('go'),lstem.stem('going'), lstem.stem('goes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b6290c-1a33-437c-86aa-9feb01b2c637",
   "metadata": {},
   "source": [
    "### 표제어 추출 Lemmatization\n",
    "- 표제어 추출의 경우는 주어진 단어를 기본형으로 변환하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b28b1eb0-9b45-4e2f-963e-9be132982a47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2b84cd0-dda6-4284-919a-efb79636f891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58b7a21c-7f45-4c4e-a7a8-dc78fa56c357",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "going\n",
      "go\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "print(lemma.lemmatize('go'))\n",
    "print(lemma.lemmatize('going'))\n",
    "print(lemma.lemmatize('goes'))\n",
    "\n",
    "print(lemma.lemmatize('going', pos = 'v')) #품사를 지정하여 뽑을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce39689c-5af9-4e43-970c-8c488ce76396",
   "metadata": {},
   "source": [
    "### 품사 태깅\n",
    "- 새공책\n",
    "- '새', '공책' / '공', '책'\n",
    "- 품사를 태깅을 해서 본래 의미가 사라지지 않게 의미있는 형태소로 만드는 것\n",
    "\n",
    "- 명사, 대명사, 동사, 형용사, 조사 등등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d30f0dea-d9d2-4eb4-8fed-7a6c2d9e9d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Machine', 'NN'), ('Learning', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('field', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('where', 'WRB'), ('computers', 'NNS'), ('learn', 'VBP'), ('from', 'IN'), ('data', 'NNS'), ('to', 'TO'), ('discover', 'VB'), ('patterns', 'NNS'), ('and', 'CC'), ('perform', 'VB'), ('tasks', 'NNS'), ('without', 'IN'), ('being', 'VBG'), ('explicitly', 'RB'), ('programmed', 'VBN'), ('.', '.'), ('It', 'PRP'), ('enables', 'VBZ'), ('computers', 'NNS'), ('to', 'TO'), ('learn', 'VB'), ('and', 'CC'), ('improve', 'VB'), ('from', 'IN'), ('experience', 'NN'), ('automatically', 'RB'), ('.', '.'), ('In', 'IN'), ('general', 'JJ'), (',', ','), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('machine', 'NN'), ('learning', 'NN'), ('involves', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(sp1)\n",
    "print(nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f4564d9-eff3-42cc-b876-4e8e4b7d0d83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 필요한 것들만 추출\n",
    "want_tag = ['NN']\n",
    "\n",
    "NN_tag = [word for word, tag in nltk.pos_tag(tokens) if tag in want_tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f54140df-163b-491a-9e51-9b032cb1efd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine',\n",
       " 'field',\n",
       " 'intelligence',\n",
       " 'experience',\n",
       " 'process',\n",
       " 'machine',\n",
       " 'learning']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b2e2fd-8c4a-4b1a-bf89-938347d2644a",
   "metadata": {},
   "source": [
    "### 한글 형태소 분석\n",
    "\n",
    "- KoNLPy 패키지를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc1a2e2e-9a34-415c-a4bf-4e80cc587de7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'konlpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkonlpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Okt\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'konlpy'"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78910c11-80e8-440a-a064-1dbd802cb004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
