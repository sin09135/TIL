# 머신러닝

## 개요

- 지도학습
  - 회귀 : 선형/비선형, 경사하강법,bias and Variance Trade off
  - 분류 : Decision Tree, Linear discriminant Analysis 등
  - 앙상블 : Bagging, Boosting

- 비지도학습
  - 차원 축소 ( 전처리 과정에서 사용 ) : Principal Component Analysis(PCA) , Singular Value Decomposition(SVD)
  - 군집화(Clustering)
    - K means , Mean Shift, Gaussian Mixture Model, DMMS

- 강화학습



## 머신러닝 딥러닝 비교

**머신러닝** : 통계적인 방법, 모델 학습

**딥러닝** : 경사하강법, 모델학습(신경망 기반)

- 파라미터가 많음

머신러닝을 이해해야 딥러닝을 이해할 수 있다.



## 지도학습

### 회귀

- 입력값 : 실수형(연속값), 범주형(이산값) 모두 가능
- 출력값 : **실수형(연속값)**
- 모델 형태 : 일반적인 함수 형태(1차 함수)



### 분류

- 입력값 : 실수형, 범주형 모두 가능
- 출력값 : **범주형**
- 모델 형태 : 이진 분류 > 시그모이드, 다중분류 > 소프트맥스 함수 포함

----

### 용어

#### 데이터의 구성

- 데이터는 피쳐와 라벨로 구성됨

| 혈압(피쳐, 독립변수) | 몸무게(피쳐, 독립변수) | 나이(피쳐, 독립변수) | 지병 (라벨, 종속변수) |
| -------------------- | ---------------------- | -------------------- | --------------------- |

- 라벨(Label)을 y로 표기 
  - o,x 라면 분류문제, 실수형이라면 회귀)
- 피쳐(Feature)
  - 행렬로 표현
  - 데이터의 특징, 항목을 의미



#### 모델

- 파라미터( = weight, 가중치)
  -  주어진 데이터 말고, 모델(함수)이 가지고 있는 학습가능한 파라미터



- 하이퍼 파라미터
  - 모델 학습에 있어서, **인간이 정해야 하는 변수**들
  - 학습률, 배치 크기 등



- 입력값(Input) vs Output(출력값)
  -  Input : x 값, 피쳐 부분
  - Output : y 값, 모델로 부터 출력되는 예측값



- 선형 vs 비선형 모델
  - Linear regression ( 선형 회귀) : 파라미터를 선형 결합식으로 표현
  - NonLinear regression ( 비선형 회귀) : 선형 결합식으로 표현 불가



## 머신러닝 기초 수학

### 함수

- 두 집합 사이의 관계 혹은 규칙  **y = f(x)**
- 일차함수  y = ax + b (a : 기울기, b = y 절편)
- 이차함수 y = a(x - p)2 + q(a != 0)



### 순간변화율( = 접선의 기울기)

- x의 값이 미세하게 변화했을 때, y의 변화율
- x변화 후 - x변화 전 / wusghkdbf



### 미분

- 함수를 미분한다는 것은 함수의 순간 변화율을 구하는 개념 
- **f'(x) = a**



### 함수의 최솟값

- 함수의 최솟값에서의 미분값(순간변화율)은 항상 0임
-  파라미터의 최적값을 구할 수 있음
- 손실함수 : (y - y^)2 손실값 최소를 구함



### 지수함수

- y = ax(a != 1, a > 0)



### 자연 상수

<img src="/Users/kimsinwoo/Downloads/자연상수e.svg" style="zoom:25%;" />

- ''자연 로그의 밑'' 또는 " 오일러의 수" 등으로 불림
- 미분을 취했을 때 자기자신이 됨



### 시그모이드 함수

- 이진 분류 문제를 위한 비선형 함수
- 함수의 출력값은 항상 0 이상,  1 이하이며, 중앙 출려값은 0.5 임



### 소프트맥스 함수

- 다중 분류 문제를 위한 비선형 함수



### 로그 함수 

- y = logax 
- log 의 밑이 안 적혀있으면 밑이 자연 상수 e



## 회귀

**모델 정의**

> 가장 먼저 데이터를 보면, 피쳐와 라벨을 분류하고 라벨의 데이터 타입을 보고 회귀/분류인지 구분
>
> 피처의 개수에 따라서 모델을 정리한다.

### 단순 선형 회귀(simple linear regression)

- 피처의 종류가 한 개인 데이터에 대한 회귀 모델
- y = w0 + w1x



### 다중 선형 회귀(multiple linear regression)

- 피처의 종류가 여러 개인 데이터에 대한 회귀 모델
- y = w0 + w1x + w2x2 ...



### 다항 회귀(Polynomial regression)

- 독립변수(피처)의 차수를 높인 회귀 모델

---

### 최적의 파라미터 결정

#### Parameter

**optimal***  = 데이터를 가장 잘 표현

- 편미분
  - 원하는 변수에 대해서만 미분
  - 그 외의 모든 것들은 상수 취급



- 연쇄법칙(chain rule)
  - 새로운 상수를 가지고 미분



#### 손실함수 정의

#### 회귀 

- MSE (Mean Squared Error, MSE)
  - 오차의 제곱의 평균



**파라미터 구하는 방법**

#### 최소제곱법(least square method)

- 최적의 파라미터를 구할 수 있는 방법으로, 데이터에 대한 오차를 최소화 하도록 함
- a 와 b에 대한 최적값 구할 수 있음(완벽한 해)



**복잡한 함수의 경우 > 최소제곱법으로 해결 어려움**

#### 경사하강법(gradient descent)

- 손실 함수의 값을 최소화 시키는 방향으로 파라미터를 업데이트 한다는 뜻
- 함수의 최솟값은 무조건 순간 변화율 = 0  이다.
- 손실함수에 대한 미분값이 0이 되는 방향으로 파라미터의 업데이트 방향을 결정



## 4. 편향과 분산

### Training data vs Test data



### 데이터의 분할

- 입력된 데이터는 학습 데이터와 평가 데이터로 나눌 수 잇음
- 학습 데이터 : 모델 학습에 사용
- 평가 데이터 : 오직 모델 평가만을 위해 사용





### 평가데이터

- 학습 데이터와 평가 데이터는 같은 분포를 가지는가 ?
  - 랜덤하게 분리하게 되면 일반적으로 비슷한 분포를 가짐
- 평가 데이터는 어느 정도 크기를 가져야 하는가?



### Bias and Varience Trade-off

#### 모델의 복잡도

- 선형에서 비선형 모델로 갈수록 복잡도 증가
- 모델이 복잡해질수록 학습데이터를 다 완벽하게 학습함
- 발생가능한 문제
  - 데이터가 많은 상황(Under - fitting)
  - 데이터가 적은 상황(Over -  fitting, 과적합
    - 무조건 파라미터 수를 늘리면 안된다



#### 편향과 분산

- 편향은 under fitting 과 관련
- 분산은 over fitting 과 관련있는 개념



<img src="/Users/kimsinwoo/Downloads/편향과 분산.png" style="zoom:50%;" />



### 모델의 복잡도를 키우고, 과적합을 막으려면?

#### 1. 검증 데이터셋 활용

| Train data | Valid Data | Test data |
| ---------- | ---------- | --------- |
| 80%        | 10%        | 10%       |

**Valid Data** : 학습 중간에 계속해서 평가를 하고, 가장 좋은 성능의 파라미터를 저장해 둠(과적합 예방)



##### LOOCV

- 랜덤으로 생성된 검증 데이터셋 하나는 편향된 결과를 줄 수 있듬
- 검증 데이터셋에 포함된 샘플들은 모델이 학습할 수 없음
- 검증 데이터셋을 n 번 검증해서 평균으로 가장 최적화된 파라미터를 저장함
- 단점 : 계산 비용이 큼

![](/Users/kimsinwoo/Downloads/loocv.png)



#### K-Fold 교차 검증

- LOOCV 의 계산 비용 문제를 해결하기 위해 **K개의 파트로 나누어 검증**
- Valid data = n/k
- K 의 값이 커지면? (k -> n)
  - 학습 데이터 수 커짐
  - Bias 작아짐, Variance 커짐
  - 계산 비용이 커짐





### 2. 손실함수 최소화

#### 정규화 손실 함수

- 모델의 복잡도가 커진다 == 모델의 파라미터 수 증가
- 모델의 복잡도가 커질수록 과적합이 발생할 가능성이 커진다.
- 복잡도가 큰 모델을 정의하고, 그 중 중요한 파라미터만 학습
- 필요없는 파라미터는 0으로 만듦



#### 정규화 종류

- Ridge 회귀(L2) > 일반적으로 많이 사용 - 원 형태

  - MSE 손실을 줄이지 못하면 패널티 항의 손실값이 더 크게 작용함

  - 람다는 정규화의 영향을 조절하는 파라미터

  - 정규화 식이 **제곱의 합**으로 표현됨

    

- Lasso 회귀(L1) - 마름모 형태

  - MSE 손실을 줄이지 못하면 패널티 항의 손실값이 더 크게 작용함
  - 람다는 정규화의 영향을 조절하는 파라미터
  - 정규화 식이 **절댓값의 합**으로 표현됨

 